{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aDlGjjxMrCEk",
   "metadata": {
    "id": "aDlGjjxMrCEk"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cwsv4JwZcV8c",
   "metadata": {
    "id": "cwsv4JwZcV8c"
   },
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q datasets\n",
    "!pip install -q monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0fa34",
   "metadata": {
    "id": "80b0fa34"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "import monai\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "from transformers import SamProcessor, SamModel\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wvnNptFYqUW2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvnNptFYqUW2",
    "outputId": "507a75cd-6bc4-4951-f07f-18126ef69004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b966008",
   "metadata": {
    "code_folding": [
     2,
     14,
     18,
     22,
     34,
     51,
     72,
     79,
     86,
     99,
     107,
     115,
     122,
     129,
     131,
     136,
     140,
     160
    ],
    "id": "5b966008"
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def load_image(path):\n",
    "\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "def load_mask(path):\n",
    "\n",
    "    return Image.open(path).convert(\"L\")\n",
    "\n",
    "def transform(example):\n",
    "\n",
    "    # Images\n",
    "    image = load_image(example['image_path'])\n",
    "    mask = load_mask(example['mask_path'])\n",
    "\n",
    "    # Keep images as PIL images\n",
    "    example['image'] = image\n",
    "    example['mask'] = mask\n",
    "\n",
    "    return example\n",
    "\n",
    "def load_salient_object_dataset(image_dir, mask_dir):\n",
    "\n",
    "    # List all images and masks\n",
    "    images = sorted([os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.jpg')])\n",
    "    masks = sorted([os.path.join(mask_dir, file) for file in os.listdir(mask_dir) if file.endswith('.png')])\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'image_path': images, 'mask_path': masks})\n",
    "\n",
    "    # Create a Dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Apply transformations\n",
    "    dataset = dataset.map(transform)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def pad_image_to_square(image, desired_size = 1000):\n",
    "\n",
    "    # Determine the number of channels (3 for RGB, 1 for L)\n",
    "    if image.mode == 'RGB':\n",
    "        fill_color = (0, 0, 0)  # Black for RGB\n",
    "\n",
    "    #\n",
    "    elif image.mode == 'L':\n",
    "        fill_color = 0  # Black for grayscale\n",
    "\n",
    "    # Calculate padding size\n",
    "    old_size = image.size\n",
    "    delta_w = desired_size - old_size[0]\n",
    "    delta_h = desired_size - old_size[1]\n",
    "    padding = (delta_w // 2, delta_h // 2, delta_w - (delta_w // 2), delta_h - (delta_h // 2))\n",
    "\n",
    "    # Pad and return\n",
    "    new_im = ImageOps.expand(image, padding, fill = fill_color)\n",
    "\n",
    "    return new_im\n",
    "\n",
    "def resize_image(image, new_size=(256, 256)):\n",
    "\n",
    "    # Resize the image\n",
    "    resized_im = image.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    return resized_im\n",
    "\n",
    "def convert_grayscale(image):\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    grayscale_image = image.convert('L')\n",
    "\n",
    "    return grayscale_image\n",
    "\n",
    "def convert_to_binary_mask_and_back(mask_pil, threshold=127):\n",
    "\n",
    "    # Convert PIL Image to NumPy array\n",
    "    mask_array = np.array(mask_pil)\n",
    "\n",
    "    # Apply threshold to convert to binary mask\n",
    "    binary_mask = (mask_array > threshold).astype(np.uint8)\n",
    "\n",
    "    # Convert binary mask back to PIL Image\n",
    "    binary_mask_pil = Image.fromarray(binary_mask)  # Multiply by 255 to get back to 0-255 range\n",
    "\n",
    "    return binary_mask_pil\n",
    "\n",
    "def resize_transform(example):\n",
    "\n",
    "    # Resize the image and the mask\n",
    "    example['image'] = resize_image(example['image'])\n",
    "    example['mask'] = resize_image(example['mask'])\n",
    "\n",
    "    return example\n",
    "\n",
    "def pad_transform(example):\n",
    "\n",
    "    # Pad the image and the mask\n",
    "    example['image'] = pad_image_to_square(example['image'])\n",
    "    example['mask'] = pad_image_to_square(example['mask'])\n",
    "\n",
    "    return example\n",
    "\n",
    "def grayscale_transform(example):\n",
    "\n",
    "    example['image'] = convert_grayscale(example['image'])\n",
    "    example['mask'] = example['mask']\n",
    "\n",
    "    return example\n",
    "\n",
    "def binary_mask_transform(example):\n",
    "\n",
    "    example['image'] = example['image']\n",
    "    example['mask'] = convert_to_binary_mask_and_back(example['mask'])\n",
    "\n",
    "    return example\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, processor):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        ground_truth_mask = np.array(item[\"mask\"])\n",
    "\n",
    "        # get bounding box prompt\n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "\n",
    "        # prepare image and prompt for the model\n",
    "        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension which the processor adds by default\n",
    "        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "\n",
    "        # add ground truth segmentation\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "        return inputs\n",
    "\n",
    "def get_bounding_box(ground_truth_map):\n",
    "\n",
    "    # get bounding box from mask\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "\n",
    "    # add perturbation to bounding box coordinates\n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "    bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    return bbox\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872b841",
   "metadata": {
    "id": "5872b841"
   },
   "source": [
    "# Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZLcrT5dQWOSp",
   "metadata": {
    "id": "ZLcrT5dQWOSp"
   },
   "outputs": [],
   "source": [
    "# Load processor\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0b10d",
   "metadata": {
    "id": "a3d0b10d"
   },
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "\n",
    "train_image_path = 'CarDD/CarDD_SOD/CarDD-TR/CarDD-TR-Image'\n",
    "train_mask_path = 'CarDD/CarDD_SOD/CarDD-TR/CarDD-TR-Mask'\n",
    "train_dataset = load_salient_object_dataset(train_image_path, train_mask_path) # Load images and masks\n",
    "train_dataset = train_dataset.map(pad_transform)  # Pad to 1000,1000 square\n",
    "train_dataset = train_dataset.map(resize_transform) # Resize to 256, 256\n",
    "train_dataset = train_dataset.map(binary_mask_transform) # Convert non-binary masks to binary masks\n",
    "train_dataset = SAMDataset(dataset = train_dataset, processor = processor) # Format using Transformers library\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 8, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zcZza8i1opvm",
   "metadata": {
    "id": "zcZza8i1opvm"
   },
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "\n",
    "test_image_path = 'CarDD/CarDD_SOD/CarDD-TE/CarDD-TE-Image'\n",
    "test_mask_path = 'CarDD/CarDD_SOD/CarDD-TE/CarDD-TE-Mask'\n",
    "test_dataset = load_salient_object_dataset(test_image_path, test_mask_path) # Load images and masks\n",
    "test_dataset = test_dataset.map(pad_transform)  # Pad to 1000,1000 square\n",
    "test_dataset = test_dataset.map(resize_transform) # Resize to 256, 256\n",
    "test_dataset = test_dataset.map(binary_mask_transform) # Convert non-binary masks to binary masks\n",
    "test_dataset = SAMDataset(dataset = test_dataset, processor = processor) # Format using Transformers library\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 8, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b2fb0",
   "metadata": {
    "id": "9f6b2fb0"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee962f83",
   "metadata": {
    "id": "ee962f83"
   },
   "outputs": [],
   "source": [
    "# Set up\n",
    "\n",
    "# Load model\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "for name, param in model.named_parameters():\n",
    "\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.mask_decoder.parameters(), lr = 1e-5, weight_decay = 0)\n",
    "\n",
    "# Loss function\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid = True, squared_pred = True, reduction = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sFi6zNheO_Gj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sFi6zNheO_Gj",
    "outputId": "205ffdf3-6c5a-4fa6-d9c9-7fd193ca9a0e"
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=batch[\"pixel_values\"].to(device), input_boxes=batch[\"input_boxes\"].to(device), multimask_output=False)\n",
    "\n",
    "        # compute loss\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "        # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    # After training, switch to evaluation mode for validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        for batch in tqdm(test_dataloader):\n",
    "\n",
    "            # forward pass\n",
    "            val_outputs = model(pixel_values = batch[\"pixel_values\"].to(device), input_boxes = batch[\"input_boxes\"].to(device), multimask_output = False)\n",
    "\n",
    "            # compute loss\n",
    "            val_predicted_masks = val_outputs.pred_masks.squeeze(1)\n",
    "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "            val_loss = seg_loss(val_predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Calculate and print the average losses for this epoch\n",
    "    train_loss = mean(epoch_losses)\n",
    "    val_loss = mean(val_losses)\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Training Mean loss: {train_loss}')\n",
    "    print(f'Validation Mean loss: {val_loss}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
