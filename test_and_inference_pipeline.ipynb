{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjXh-7FcgnQ8"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WPwQLx1g3MS"
   },
   "source": [
    "## Setup YoloV8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LErM6bmUgyjo"
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2ozrCNkmPXi"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krIJvNbJoMjw"
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "yolo_model = YOLO('/CarDD/yolov8_weights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0rn3GqxjyEs"
   },
   "source": [
    "## Setup SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vfvi-MLEj6uL"
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q datasets\n",
    "!pip install -q monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGc3nfacmVV8"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import monai\n",
    "from statistics import mean\n",
    "from transformers import SamProcessor, SamModel\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as Dataset_torch\n",
    "from torch.nn.functional import threshold, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7FL2kxHoffA"
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "sam_model = SamModel.from_pretrained(\"yjmsvma/car_sam\")\n",
    "sam_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXFSdIZnlC43"
   },
   "source": [
    "## Setup GroundingDINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGhyP0YUlIpi"
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "%cd {HOME}\n",
    "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
    "%cd {HOME}/GroundingDINO\n",
    "!pip install -q -e .\n",
    "!pip install -q roboflow\n",
    "\n",
    "CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "print(CONFIG_PATH, \"; exist:\", os.path.isfile(CONFIG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnftiaZWlpKO"
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "%cd {HOME}\n",
    "!mkdir {HOME}/weights\n",
    "%cd {HOME}/weights\n",
    "\n",
    "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
    "\n",
    "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
    "WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n",
    "print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96CekfFGls0a"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "%cd {HOME}/GroundingDINO\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtqlT8lel-w4"
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "groundingdino_model = load_model(CONFIG_PATH, WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yz5G86IJL8GQ"
   },
   "source": [
    "## Set up OWL-VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcIlAoykL_9Z"
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "!pip install Pillow\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBtsDTuOMO05"
   },
   "outputs": [],
   "source": [
    " # Imports\n",
    "\n",
    "import cv2\n",
    "import skimage\n",
    "import numpy as np\n",
    "from matplotlib import rcParams\n",
    "from transformers.image_utils import ImageFeatureExtractionMixin\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chNBNkgFMqzd"
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "owlvit_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-large-patch14\")\n",
    "owlvit_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-large-patch14\")\n",
    "\n",
    "# Set model in evaluation mode\n",
    "owlvit_model = owlvit_model.to(device)\n",
    "owlvit_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nanRGsafmBKv"
   },
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBuMipe2mCxT"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import supervision as sv\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "\n",
    "from cv2 import imread, imwrite\n",
    "\n",
    "label_prompt_conversion = {'dent':'dented damage',\n",
    "                           'scratch':'scratched damage',\n",
    "                           'crack':'cracked damage',\n",
    "                           'glass shatter':'shattered glass',\n",
    "                           'lamp broken':'broken lamp',\n",
    "                           'tire flat':'flat tire'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo4VOs5v35MX"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cwFdm_mtGZe"
   },
   "outputs": [],
   "source": [
    "def pad_to_square(binary_mask):\n",
    "\n",
    "    \"\"\"\n",
    "    Pad a binary mask numpy array to make it square.\n",
    "\n",
    "    Parameters:\n",
    "    binary_mask (numpy.ndarray): A 2D binary mask array.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A padded square binary mask.\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = binary_mask.shape\n",
    "    if height == width:\n",
    "        return binary_mask\n",
    "\n",
    "    # Determine the size to pad to (the larger dimension of the mask)\n",
    "    square_size = max(height, width)\n",
    "\n",
    "    # Calculate padding sizes\n",
    "    pad_height = (square_size - height) // 2\n",
    "    pad_width = (square_size - width) // 2\n",
    "\n",
    "    # Pad the array\n",
    "    padded_mask = np.pad(binary_mask,\n",
    "                         pad_width=((pad_height, square_size - height - pad_height),\n",
    "                                    (pad_width, square_size - width - pad_width)),\n",
    "                         mode='constant', constant_values=0)\n",
    "\n",
    "    return padded_mask\n",
    "\n",
    "def resize_mask(mask, new_size):\n",
    "\n",
    "    \"\"\"\n",
    "    Resize a square mask to a new size.\n",
    "\n",
    "    Parameters:\n",
    "    mask (numpy.ndarray): A 2D square mask array.\n",
    "    new_size (int): The size of the new square mask.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A resized square mask.\n",
    "    \"\"\"\n",
    "\n",
    "    resized_mask = cv2.resize(mask, (new_size, new_size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return resized_mask\n",
    "\n",
    "def calculate_iou_mask(mask1, mask2):\n",
    "\n",
    "    \"\"\"Calculate the Intersection over Union (IoU) of two masks.\"\"\"\n",
    "\n",
    "    intersection = np.logical_and(mask1, mask2)\n",
    "    union = np.logical_or(mask1, mask2)\n",
    "    iou = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    return iou\n",
    "\n",
    "def map_values(array, values_list):\n",
    "\n",
    "    \"\"\"\n",
    "    Map values in the array to 1 if they are in values_list, otherwise to 0.\n",
    "\n",
    "    Parameters:\n",
    "    - array: A NumPy array.\n",
    "    - values_list: A list of values to be mapped to 1.\n",
    "\n",
    "    Returns:\n",
    "    - A NumPy array where all elements in values_list are 1, and others are 0.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isin(array, values_list).astype(int)\n",
    "\n",
    "def pad_image_to_square(image, desired_size = 1000):\n",
    "\n",
    "    # Determine the number of channels (3 for RGB, 1 for L)\n",
    "    if image.mode == 'RGB':\n",
    "        fill_color = (0, 0, 0)  # Black for RGB\n",
    "\n",
    "    #\n",
    "    elif image.mode == 'L':\n",
    "        fill_color = 0  # Black for grayscale\n",
    "\n",
    "    # Calculate padding size\n",
    "    old_size = image.size\n",
    "    delta_w = desired_size - old_size[0]\n",
    "    delta_h = desired_size - old_size[1]\n",
    "    padding = (delta_w // 2, delta_h // 2, delta_w - (delta_w // 2), delta_h - (delta_h // 2))\n",
    "\n",
    "    # Pad and return\n",
    "    new_im = ImageOps.expand(image, padding, fill = fill_color)\n",
    "\n",
    "    return new_im\n",
    "\n",
    "def resize_image(image, new_size):\n",
    "\n",
    "    # Resize the image\n",
    "    resized_im = image.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    return resized_im\n",
    "\n",
    "def visualize_yolo_predictions(image, predictions, confidence_threshold):\n",
    "\n",
    "    \"\"\"\n",
    "    Visualize YOLO predictions on the image using Ultralytics library.\n",
    "\n",
    "    Parameters:\n",
    "    image (PIL.Image or numpy.ndarray): The image on which to draw the predictions.\n",
    "    predictions (torch.Tensor): The predictions output by the model.\n",
    "    conf_threshold (float): Confidence threshold to filter out lower-confidence predictions.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    class_names = {0: 'dent',\n",
    "                   1: 'scratch',\n",
    "                   2: 'crack',\n",
    "                   3: 'glass shatter',\n",
    "                   4: 'lamp broken',\n",
    "                   5: 'tire flat'}\n",
    "\n",
    "    # Convert image to numpy array if it's a PIL Image\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        image = np.array(image)\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Each prediction consists of [x1, y1, x2, y2, confidence, class]\n",
    "    for pred in predictions:\n",
    "\n",
    "        boxes = pred.boxes.data.tolist()  # Boxes object for bbox outputs\n",
    "\n",
    "        for box in boxes:\n",
    "\n",
    "            x1, y1, x2, y2, conf, class_id = box\n",
    "\n",
    "            class_name = class_names[class_id]\n",
    "\n",
    "            if conf >= confidence_threshold:\n",
    "\n",
    "                width, height = x2 - x1, y2 - y1\n",
    "                rect = Rectangle((x1, y1), width, height, linewidth = 2, edgecolor = 'r', facecolor = 'none')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "                plt.text(x1, y1,\n",
    "                         f'Class: {class_name}, Conf: {conf:.2f}', color='white',\n",
    "                         bbox = dict(facecolor = 'red', alpha = 0.5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_yolo_pred_box(predictions, confidence_threshold):\n",
    "\n",
    "    class_names = {0: 'dent',\n",
    "                   1: 'scratch',\n",
    "                   2: 'crack',\n",
    "                   3: 'glass shatter',\n",
    "                   4: 'lamp broken',\n",
    "                   5: 'tire flat'}\n",
    "\n",
    "    bbox_dict = {}\n",
    "    scores_dict = {}\n",
    "    for pred in predictions:\n",
    "\n",
    "        boxes = pred.boxes.data.tolist()  # Boxes object for bbox outputs\n",
    "\n",
    "        for box in boxes:\n",
    "\n",
    "            x1, y1, x2, y2, conf, class_id = box\n",
    "\n",
    "            class_name = class_names[class_id]\n",
    "\n",
    "            if class_name not in bbox_dict:\n",
    "                bbox_dict[class_name] = []\n",
    "\n",
    "            if class_name not in scores_dict:\n",
    "                scores_dict[class_name] = []\n",
    "\n",
    "            if conf >= confidence_threshold:\n",
    "\n",
    "                bbox_dict[class_name].append([x1, y1, x2, y2])\n",
    "                scores_dict[class_name].append(conf)\n",
    "\n",
    "    # nms_bbox_dict = {}\n",
    "    # for class_name, bboxes in bbox_dict.items():\n",
    "\n",
    "    #     nms_bboxes = non_max_suppression(bboxes, scores_dict[class_name], 0.5)\n",
    "    #     nms_bbox_dict[class_name] = nms_bboxes\n",
    "\n",
    "    return bbox_dict\n",
    "\n",
    "def transform_bounding_box(bbox, orig_width, orig_height, target_size):\n",
    "\n",
    "    \"\"\"\n",
    "    Transform the bounding box coordinates based on image transformations.\n",
    "\n",
    "    Parameters:\n",
    "    bbox (tuple): A tuple (x_min, y_min, x_max, y_max) representing the original bounding box.\n",
    "    orig_width (int): Original width of the image.\n",
    "    orig_height (int): Original height of the image.\n",
    "    target_size (int): The size of the transformed (square) image. Default is 640.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Transformed bounding box coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate padding to make the image square\n",
    "    pad_x = (max(orig_width, orig_height) - orig_width) / 2\n",
    "    pad_y = (max(orig_width, orig_height) - orig_height) / 2\n",
    "\n",
    "    # Scale factor (since the image is scaled down to target_size x target_size)\n",
    "    scale = target_size / max(orig_width, orig_height)\n",
    "\n",
    "    # Transform the bounding box coordinates\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    x_min_padded = (x_min + pad_x) * scale\n",
    "    y_min_padded = (y_min + pad_y) * scale\n",
    "    x_max_padded = (x_max + pad_x) * scale\n",
    "    y_max_padded = (y_max + pad_y) * scale\n",
    "\n",
    "    return [int(x_min_padded), int(y_min_padded), int(x_max_padded), int(y_max_padded)]\n",
    "\n",
    "def show_mask(mask, ax, random_color = False):\n",
    "\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def convert_to_binary_mask_and_back(mask_pil, threshold = 127):\n",
    "\n",
    "    # Convert PIL Image to NumPy array\n",
    "    mask_array = np.array(mask_pil)\n",
    "\n",
    "    # Apply threshold to convert to binary mask\n",
    "    binary_mask = (mask_array > threshold).astype(np.uint8)\n",
    "\n",
    "    return binary_mask\n",
    "\n",
    "def convert_to_pixels(box, image_width, image_height):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert normalized bounding box coordinates to pixel coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    box (tuple): A tuple (x_center, y_center, width, height) of the bounding box,\n",
    "                 with values between 0.0 and 1.0.\n",
    "    image_width (int): Width of the image.\n",
    "    image_height (int): Height of the image.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple (x_min, y_min, x_max, y_max) of the bounding box in pixel coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    x_center, y_center, width, height = box\n",
    "    x_min = int((x_center - width / 2) * image_width)\n",
    "    y_min = int((y_center - height / 2) * image_height)\n",
    "    x_max = int((x_center + width / 2) * image_width)\n",
    "    y_max = int((y_center + height / 2) * image_height)\n",
    "\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "def bounding_box_to_mask(bbox, image_shape):\n",
    "    \"\"\"\n",
    "    Convert a bounding box to a mask.\n",
    "\n",
    "    Parameters:\n",
    "    - bbox: A tuple (x_min, y_min, x_max, y_max) representing the bounding box.\n",
    "    - image_shape: The shape of the image (height, width).\n",
    "\n",
    "    Returns:\n",
    "    - mask: A binary mask where pixels inside the bounding box are 1, others are 0.\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "    mask[int(y_min):int(y_max), int(x_min):int(x_max)] = 1\n",
    "\n",
    "    return mask\n",
    "\n",
    "def elementwise_addition(arrays):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform element-wise addition of a list of NumPy arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - arrays: A list of NumPy arrays of the same shape.\n",
    "\n",
    "    Returns:\n",
    "    - A NumPy array containing the element-wise sum of the input arrays.\n",
    "    \"\"\"\n",
    "    # Check if the list is empty\n",
    "    if not arrays:\n",
    "        raise ValueError(\"The list of arrays is empty.\")\n",
    "\n",
    "    # Check if all arrays have the same shape\n",
    "    shape = arrays[0].shape\n",
    "    if not all(arr.shape == shape for arr in arrays):\n",
    "        raise ValueError(\"All arrays must have the same shape.\")\n",
    "\n",
    "    # Perform element-wise addition\n",
    "    result = np.sum(arrays, axis=0)\n",
    "\n",
    "    return result\n",
    "\n",
    "def map_values_to_one(array, benchmark):\n",
    "\n",
    "    \"\"\"\n",
    "    Map values in the array to 1 if they are greater than or equal to 2.\n",
    "\n",
    "    Parameters:\n",
    "    - array: A NumPy array.\n",
    "\n",
    "    Returns:\n",
    "    - A NumPy array where all values >= 2 are replaced with 1.\n",
    "    \"\"\"\n",
    "\n",
    "    array[array < benchmark] = 0\n",
    "\n",
    "    return array\n",
    "\n",
    "def mask_to_bounding_box(mask):\n",
    "    \"\"\"\n",
    "    Convert a mask to a bounding box.\n",
    "\n",
    "    Parameters:\n",
    "    - mask: A binary mask (numpy array) where the object is represented by non-zero values.\n",
    "\n",
    "    Returns:\n",
    "    - bbox: A tuple (x_min, y_min, x_max, y_max) representing the bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the coordinates where the mask is not zero\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    y_min, y_max = np.where(rows)[0][[0, -1]]\n",
    "    x_min, x_max = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Return the bounding box coordinates\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "def non_max_suppression(boxes, probabilities, threshold):\n",
    "\n",
    "    \"\"\"Apply non-maximum suppression to avoid detecting the same object multiple times.\"\"\"\n",
    "\n",
    "    # Sort the boxes by their probabilities (scores)\n",
    "    sorted_indices = np.argsort(probabilities)[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while len(sorted_indices) > 0:\n",
    "\n",
    "        # Take the box with the highest score\n",
    "        current = sorted_indices[0]\n",
    "        keep.append(current)\n",
    "\n",
    "        if len(sorted_indices) == 1:\n",
    "            break\n",
    "\n",
    "        # Compute IoU of the current box with the rest\n",
    "        ious = np.array([calculate_iou_mask(bounding_box_to_mask(boxes[current], (640, 640)), bounding_box_to_mask(boxes[current], (640, 640))) for next_box in sorted_indices[1:]])\n",
    "\n",
    "        # Keep boxes with IoU less than the threshold\n",
    "        sorted_indices = sorted_indices[1:][ious < threshold]\n",
    "\n",
    "    return [boxes[i] for i in keep]\n",
    "\n",
    "def coco_poly_to_mask(polygon, image_shape):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert COCO polygon to a binary mask.\n",
    "\n",
    "    Parameters:\n",
    "    - polygon: List of lists of coordinates for the polygons.\n",
    "    - image_shape: Tuple of (height, width) for the mask.\n",
    "\n",
    "    Returns:\n",
    "    - A NumPy array representing the binary mask.\n",
    "    \"\"\"\n",
    "\n",
    "    mask_img = Image.new('L', (image_shape[1], image_shape[0]), 0)\n",
    "    for poly in polygon:\n",
    "\n",
    "        # The polygon is expected to be a list of [x1, y1, x2, y2, ..., xn, yn]\n",
    "        if len(poly) % 2 != 0:\n",
    "            raise ValueError(\"Polygon length must be even.\")\n",
    "\n",
    "        # Draw the polygon\n",
    "        ImageDraw.Draw(mask_img).polygon(poly, outline=1, fill=1)\n",
    "\n",
    "    return np.array(mask_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OudF8oiP6xkI"
   },
   "outputs": [],
   "source": [
    "# Get paths to unseen images\n",
    "image_base_path = '/CarDD/CarDD_SOD/CarDD-TE/CarDD-TE-Image/'\n",
    "images = os.listdir(image_base_path)\n",
    "\n",
    "image_name = images[images.index('003649.jpg')]\n",
    "image_source, image = load_image(image_base_path + image_name)\n",
    "image_source_pil = Image.fromarray(image_source)\n",
    "image_source_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE2kw7PJpMHL"
   },
   "source": [
    "# Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuXjXP4d_IwL"
   },
   "outputs": [],
   "source": [
    "image_base_path = '/CarDD/CarDD_COCO/test/'\n",
    "images = os.listdir(image_base_path)\n",
    "\n",
    "categories = {1:'dent', 2:'scratch', 3:'crack', 4:'glass shatter', 5:'lamp broken', 6:'tire flat'}\n",
    "\n",
    "with open('/CarDD/CarDD_COCO/annotations/instances_test.json', 'r') as f:\n",
    "    test_annotations = json.load(f)['annotations']\n",
    "\n",
    "annFile = '/CarDD/CarDD_COCO/annotations/instances_test.json'\n",
    "coco = COCO(annFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGrd2ySZcJIp"
   },
   "source": [
    "## Inference on single image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "zb1dn73cKV5C"
   },
   "outputs": [],
   "source": [
    "# Select image and run models\n",
    "image_name = images[1]\n",
    "ok = True\n",
    "if ok:\n",
    "\n",
    "    image_path = image_base_path + image_name\n",
    "    print(image_path)\n",
    "\n",
    "    image_source, image = load_image(image_path)\n",
    "    image_source_pil = Image.fromarray(image_source)\n",
    "    image_id = int(image_name.replace('.jpg', ''))\n",
    "    image_annotations = [annotation for annotation in test_annotations if annotation['image_id'] == image_id]\n",
    "\n",
    "    groundedtruth_segmentations = {}\n",
    "    for annotation in image_annotations:\n",
    "\n",
    "        ground_truth_label = categories[annotation['category_id']]\n",
    "        ground_truth_mask = coco_poly_to_mask(annotation['segmentation'], image_source.shape[:2])\n",
    "\n",
    "        padded_ground_truth_mask = pad_to_square(ground_truth_mask)\n",
    "        resized_ground_truth_mask = resize_mask(padded_ground_truth_mask, 256)\n",
    "\n",
    "        if ground_truth_label not in groundedtruth_segmentations:\n",
    "            groundedtruth_segmentations[ground_truth_label] = np.array(resized_ground_truth_mask)\n",
    "\n",
    "        else:\n",
    "            groundedtruth_segmentations[ground_truth_label] += np.array(resized_ground_truth_mask)\n",
    "    print('existing', list(groundedtruth_segmentations.keys()))\n",
    "\n",
    "    total_ground_truth = np.zeros((256, 256))\n",
    "    for label, mask in groundedtruth_segmentations.items():\n",
    "        total_ground_truth += mask\n",
    "    total_ground_truth = map_values(total_ground_truth, [1])\n",
    "\n",
    "    ##### Get YOLOv8 bounding boxes #####\n",
    "\n",
    "    # Pad image to make square\n",
    "    padded_image = pad_image_to_square(image_source_pil, desired_size = 1000)\n",
    "\n",
    "    # Resize to YOLOv8 size\n",
    "    yolo_resized_image = resize_image(padded_image, new_size = (640, 640))\n",
    "\n",
    "    # YOLOv8 inference\n",
    "    yolo_prediction = yolo_model([yolo_resized_image])\n",
    "\n",
    "    # Get predicted bounding boxes with confidence threshhold\n",
    "    yolo_bboxes = get_yolo_pred_box(yolo_prediction, confidence_threshold = 0.5)\n",
    "\n",
    "    ##### Get GroundingDINO bounding boxes #####\n",
    "    groundingdino_bboxes = {}\n",
    "    for predicted_label, bbox_list in yolo_bboxes.items():\n",
    "\n",
    "        if predicted_label not in groundingdino_bboxes:\n",
    "            groundingdino_bboxes[predicted_label] = []\n",
    "\n",
    "        TEXT_PROMPT = label_prompt_conversion[predicted_label]\n",
    "        BOX_TRESHOLD = 0.35\n",
    "        TEXT_TRESHOLD = 0.25\n",
    "\n",
    "        boxes, logits, phrases = predict(model = groundingdino_model, image = image, caption = TEXT_PROMPT, box_threshold = BOX_TRESHOLD, text_threshold = TEXT_TRESHOLD)\n",
    "        scores = torch.sigmoid(logits).cpu().detach().numpy()\n",
    "\n",
    "        for box in boxes:\n",
    "\n",
    "            pixel_box = convert_to_pixels(box.tolist(), image_source.shape[1], image_source.shape[0])\n",
    "            resized_box = transform_bounding_box(pixel_box, image_source.shape[1], image_source.shape[0], target_size = 640)\n",
    "            groundingdino_bboxes[predicted_label].append(resized_box)\n",
    "\n",
    "    #### Get OWL_ViT bounding boxes #####\n",
    "    owlvittext_bboxes = {}\n",
    "    for predicted_label, info in yolo_bboxes.items():\n",
    "\n",
    "        text_queries = [label_prompt_conversion[predicted_label]]\n",
    "\n",
    "        # Process image and text inputs\n",
    "        owlvit_inputs = owlvit_processor(text=text_queries, images = image_source_pil.convert(\"RGB\"), return_tensors = \"pt\").to(device)\n",
    "\n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            owlvit_outputs = owlvit_model(**owlvit_inputs)\n",
    "\n",
    "        # Threshold to eliminate low probability predictions\n",
    "        score_threshold = 0.1\n",
    "\n",
    "        # Get prediction logits\n",
    "        logits = torch.max(owlvit_outputs[\"logits\"][0], dim=-1)\n",
    "        scores = torch.sigmoid(logits.values).cpu().detach().numpy()\n",
    "\n",
    "        # Get prediction labels and boundary boxes\n",
    "        owlvit_boxes = owlvit_outputs[\"pred_boxes\"][0].cpu().detach().numpy()\n",
    "\n",
    "        owlvit_results = [scores, owlvit_boxes]\n",
    "        df = pd.DataFrame(owlvit_results).T\n",
    "        sorted_df = df.sort_values(0, ascending = False)\n",
    "        filtered_df = sorted_df[sorted_df[0] > score_threshold]\n",
    "\n",
    "        owlvit_boxes = list(filtered_df[1])\n",
    "        owlvit_scores = list(filtered_df[0])\n",
    "\n",
    "        pixel_format_boxes = [convert_to_pixels(box, image_source.shape[1], image_source.shape[0]) for box in owlvit_boxes]\n",
    "        resized_bboxes = [transform_bounding_box(box, image_source.shape[1], image_source.shape[0], target_size = 640) for box in pixel_format_boxes]\n",
    "\n",
    "        owlvit_nms_boxes = non_max_suppression(resized_bboxes, owlvit_scores, 0.5)\n",
    "\n",
    "        owlvittext_bboxes[predicted_label] = resized_bboxes\n",
    "\n",
    "    ##### Get masks #####\n",
    "\n",
    "    # Resize to SAM size\n",
    "    sam_resized_image = resize_image(padded_image, new_size = (256, 256))\n",
    "\n",
    "    ##### Get YoloV8 masks #####\n",
    "    yolo_sam_segmentations = {}\n",
    "    for class_name, bboxes in yolo_bboxes.items():\n",
    "\n",
    "        for bbox in bboxes:\n",
    "\n",
    "            # Resize boundary\n",
    "            sam_resized_bbox = list(transform_bounding_box(bbox, 640, 640, target_size = 256))\n",
    "\n",
    "            # SAM inference\n",
    "            sam_inputs = sam_processor(sam_resized_image, input_boxes = [[sam_resized_bbox]], return_tensors = \"pt\").to('cuda')\n",
    "            sam_model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                sam_outputs = sam_model(**sam_inputs, multimask_output = False)\n",
    "\n",
    "            # Filter predicted map by 0.5\n",
    "            sam_seg_prob = torch.sigmoid(sam_outputs.pred_masks.squeeze(1))\n",
    "            sam_seg_prob = sam_seg_prob.cpu().numpy().squeeze()\n",
    "            sam_seg = (sam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "            if class_name not in yolo_sam_segmentations:\n",
    "                yolo_sam_segmentations[class_name] = sam_seg\n",
    "\n",
    "            else:\n",
    "                yolo_sam_segmentations[class_name] += sam_seg\n",
    "\n",
    "    #### Get GroundingDINO masks #####\n",
    "    groundingdino_sam_segmentations = {}\n",
    "    for class_name, bboxes in groundingdino_bboxes.items():\n",
    "\n",
    "        for bbox in bboxes:\n",
    "\n",
    "            # Resize boundary\n",
    "            sam_resized_bbox = list(transform_bounding_box(bbox, 640, 640, target_size = 256))\n",
    "\n",
    "            # SAM inference\n",
    "            sam_inputs = sam_processor(sam_resized_image, input_boxes = [[sam_resized_bbox]], return_tensors = \"pt\").to('cuda')\n",
    "            sam_model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                sam_outputs = sam_model(**sam_inputs, multimask_output = False)\n",
    "\n",
    "            # Filter predicted map by 0.5\n",
    "            sam_seg_prob = torch.sigmoid(sam_outputs.pred_masks.squeeze(1))\n",
    "            sam_seg_prob = sam_seg_prob.cpu().numpy().squeeze()\n",
    "            sam_seg = (sam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "            if class_name not in groundingdino_sam_segmentations:\n",
    "                groundingdino_sam_segmentations[class_name] = sam_seg\n",
    "\n",
    "            else:\n",
    "                groundingdino_sam_segmentations[class_name] += sam_seg\n",
    "\n",
    "    #### Get OWL-ViT masks #####\n",
    "    owlvit_sam_segmentations = {}\n",
    "    for class_name, bboxes in owlvittext_bboxes.items():\n",
    "\n",
    "        for bbox in bboxes:\n",
    "\n",
    "            # Resize boundary\n",
    "            sam_resized_bbox = list(transform_bounding_box(bbox, 640, 640, target_size = 256))\n",
    "\n",
    "            # SAM inference\n",
    "            sam_inputs = sam_processor(sam_resized_image, input_boxes = [[sam_resized_bbox]], return_tensors = \"pt\").to('cuda')\n",
    "            sam_model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                sam_outputs = sam_model(**sam_inputs, multimask_output = False)\n",
    "\n",
    "            # Filter predicted map by 0.5\n",
    "            sam_seg_prob = torch.sigmoid(sam_outputs.pred_masks.squeeze(1))\n",
    "            sam_seg_prob = sam_seg_prob.cpu().numpy().squeeze()\n",
    "            sam_seg = (sam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "            if class_name not in owlvit_sam_segmentations:\n",
    "                owlvit_sam_segmentations[class_name] = sam_seg\n",
    "\n",
    "            else:\n",
    "                owlvit_sam_segmentations[class_name] += sam_seg\n",
    "\n",
    "    # Combine different damage types\n",
    "    combined_damage_mask = {k:[] for k in ['dent', 'scratch', 'cracked damage', 'glass shatter', 'lamp broken', 'tire flat']}\n",
    "    for damage_type in ['dent', 'scratch', 'cracked damage', 'glass shatter', 'lamp broken', 'tire flat']:\n",
    "\n",
    "        total_mask = np.zeros((256, 256))\n",
    "\n",
    "        if damage_type in yolo_sam_segmentations:\n",
    "            total_mask += yolo_sam_segmentations[damage_type]\n",
    "\n",
    "        if damage_type in groundingdino_sam_segmentations:\n",
    "            total_mask += groundingdino_sam_segmentations[damage_type]\n",
    "\n",
    "        if damage_type in owlvit_sam_segmentations:\n",
    "            total_mask += owlvit_sam_segmentations[damage_type]\n",
    "\n",
    "        unique_mask_values = np.unique(total_mask.flatten())\n",
    "        if max(unique_mask_values) > 0:\n",
    "            normalized_mask = map_values(total_mask, [val for val in unique_mask_values if val >= int(max(unique_mask_values)/2)])\n",
    "        else:\n",
    "            normalized_mask = np.zeros((256, 256))\n",
    "\n",
    "        combined_damage_mask[damage_type] = normalized_mask\n",
    "\n",
    "    total_pred = np.zeros((256, 256))\n",
    "    for label, mask in combined_damage_mask.items():\n",
    "        total_pred += mask\n",
    "    total_pred = map_values(total_pred, [1])\n",
    "\n",
    "    total_iou = calculate_iou_mask(total_pred, total_ground_truth)\n",
    "    print(total_iou)\n",
    "\n",
    "    for label, mask in groundedtruth_segmentations.items():\n",
    "\n",
    "        if label in yolo_sam_segmentations:\n",
    "            iou = calculate_iou_mask(combined_damage_mask[label], mask)\n",
    "            print(label, iou)\n",
    "\n",
    "        else:\n",
    "            iou = None\n",
    "\n",
    "        # testing_result = {'image_name':image_name, 'label':label, 'iou':iou}\n",
    "\n",
    "        # testing_results.append(testing_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8L8E0ZUZllTP"
   },
   "outputs": [],
   "source": [
    "# Visualize damage type\n",
    "\n",
    "damage_type = 'dent'\n",
    "ok = True\n",
    "if ok:\n",
    "\n",
    "    fig, axes = plt.subplots()\n",
    "    axes.imshow(np.array(sam_resized_image))\n",
    "    show_mask(groundedtruth_segmentations[damage_type], axes)\n",
    "    axes.title.set_text(f\"Mask\")\n",
    "    axes.axis(\"off\")\n",
    "\n",
    "    try:\n",
    "        fig, axes = plt.subplots()\n",
    "        axes.imshow(np.array(sam_resized_image))\n",
    "        show_mask(yolo_sam_segmentations[damage_type], axes)\n",
    "        axes.title.set_text(f\"yolo segmentations\")\n",
    "        axes.axis(\"off\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        fig, axes = plt.subplots()\n",
    "        axes.imshow(np.array(sam_resized_image))\n",
    "        show_mask(groundingdino_sam_segmentations[damage_type], axes)\n",
    "        axes.title.set_text(f\"groundingdino segmentations\")\n",
    "        axes.axis(\"off\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        fig, axes = plt.subplots()\n",
    "        axes.imshow(np.array(sam_resized_image))\n",
    "        show_mask(owlvit_sam_segmentations[damage_type], axes)\n",
    "        axes.title.set_text(f\"owlvit segmentations\")\n",
    "        axes.axis(\"off\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        fig, axes = plt.subplots()\n",
    "        axes.imshow(np.array(sam_resized_image))\n",
    "        show_mask(combined_damage_mask[damage_type], axes)\n",
    "        axes.title.set_text(f\"combined segmentations\")\n",
    "        axes.axis(\"off\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01FsLogiddtc"
   },
   "source": [
    "## Test on full test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRiokrTgYghF"
   },
   "outputs": [],
   "source": [
    "testing_results = []\n",
    "for image_name in tqdm(images):\n",
    "\n",
    "    image_path = image_base_path + image_name\n",
    "    image_source, image = load_image(image_path)\n",
    "    image_source_pil = Image.fromarray(image_source)\n",
    "    image_id = int(image_name.replace('.jpg', ''))\n",
    "    image_annotations = [annotation for annotation in test_annotations if annotation['image_id'] == image_id]\n",
    "\n",
    "    groundedtruth_segmentations = {}\n",
    "    for annotation in image_annotations:\n",
    "\n",
    "        ground_truth_label = categories[annotation['category_id']]\n",
    "        ground_truth_mask = coco_poly_to_mask(annotation['segmentation'], image_source.shape[:2])\n",
    "\n",
    "        padded_ground_truth_mask = pad_to_square(ground_truth_mask)\n",
    "        resized_ground_truth_mask = resize_mask(padded_ground_truth_mask, 256)\n",
    "\n",
    "        if ground_truth_label not in groundedtruth_segmentations:\n",
    "            groundedtruth_segmentations[ground_truth_label] = np.array(resized_ground_truth_mask)\n",
    "\n",
    "        else:\n",
    "            groundedtruth_segmentations[ground_truth_label] += np.array(resized_ground_truth_mask)\n",
    "\n",
    "    ##### Get YOLOv8 bounding boxes #####\n",
    "\n",
    "    # Pad image to make square\n",
    "    padded_image = pad_image_to_square(image_source_pil, desired_size = 1000)\n",
    "\n",
    "    # Resize to YOLOv8 size\n",
    "    yolo_resized_image = resize_image(padded_image, new_size = (640, 640))\n",
    "\n",
    "    # YOLOv8 inference\n",
    "    yolo_prediction = yolo_model([yolo_resized_image])\n",
    "\n",
    "    # Get predicted bounding boxes with confidence threshhold\n",
    "    yolo_bboxes = get_yolo_pred_box(yolo_prediction, confidence_threshold = 0.5)\n",
    "\n",
    "    ##### Get GroundingDINO bounding boxes #####\n",
    "    groundingdino_bboxes = {}\n",
    "    for predicted_label, bbox_list in yolo_bboxes.items():\n",
    "\n",
    "        if predicted_label not in groundingdino_bboxes:\n",
    "            groundingdino_bboxes[predicted_label] = []\n",
    "\n",
    "        TEXT_PROMPT = label_prompt_conversion[predicted_label]\n",
    "        BOX_TRESHOLD = 0.35\n",
    "        TEXT_TRESHOLD = 0.25\n",
    "\n",
    "        boxes, logits, phrases = predict(model = groundingdino_model, image = image, caption = TEXT_PROMPT, box_threshold = BOX_TRESHOLD, text_threshold = TEXT_TRESHOLD)\n",
    "        scores = torch.sigmoid(logits).cpu().detach().numpy()\n",
    "\n",
    "        for box in boxes:\n",
    "\n",
    "            pixel_box = convert_to_pixels(box.tolist(), image_source.shape[1], image_source.shape[0])\n",
    "            resized_box = transform_bounding_box(pixel_box, image_source.shape[1], image_source.shape[0], target_size = 640)\n",
    "            groundingdino_bboxes[predicted_label].append(resized_box)\n",
    "\n",
    "    #### Get OWL_ViT bounding boxes #####\n",
    "    owlvittext_bboxes = {}\n",
    "    for predicted_label, info in yolo_bboxes.items():\n",
    "\n",
    "        text_queries = [label_prompt_conversion[predicted_label]]\n",
    "\n",
    "        # Process image and text inputs\n",
    "        owlvit_inputs = owlvit_processor(text=text_queries, images = image_source_pil.convert(\"RGB\"), return_tensors = \"pt\").to(device)\n",
    "\n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            owlvit_outputs = owlvit_model(**owlvit_inputs)\n",
    "\n",
    "        # Threshold to eliminate low probability predictions\n",
    "        score_threshold = 0.1\n",
    "\n",
    "        # Get prediction logits\n",
    "        logits = torch.max(owlvit_outputs[\"logits\"][0], dim=-1)\n",
    "        scores = torch.sigmoid(logits.values).cpu().detach().numpy()\n",
    "\n",
    "        # Get prediction labels and boundary boxes\n",
    "        owlvit_boxes = owlvit_outputs[\"pred_boxes\"][0].cpu().detach().numpy()\n",
    "\n",
    "        owlvit_results = [scores, owlvit_boxes]\n",
    "        df = pd.DataFrame(owlvit_results).T\n",
    "        sorted_df = df.sort_values(0, ascending = False)\n",
    "        filtered_df = sorted_df[sorted_df[0] > score_threshold]\n",
    "\n",
    "        owlvit_boxes = list(filtered_df[1])\n",
    "        owlvit_scores = list(filtered_df[0])\n",
    "\n",
    "        pixel_format_boxes = [convert_to_pixels(box, image_source.shape[1], image_source.shape[0]) for box in owlvit_boxes]\n",
    "        resized_bboxes = [transform_bounding_box(box, image_source.shape[1], image_source.shape[0], target_size = 640) for box in pixel_format_boxes]\n",
    "\n",
    "        owlvit_nms_boxes = non_max_suppression(resized_bboxes, owlvit_scores, 0.5)\n",
    "\n",
    "        owlvittext_bboxes[predicted_label] = resized_bboxes\n",
    "\n",
    "    ##### Get masks #####\n",
    "\n",
    "    # Resize to SAM size\n",
    "    sam_resized_image = resize_image(padded_image, new_size = (256, 256))\n",
    "\n",
    "    ##### Get YoloV8 masks #####\n",
    "    yolo_sam_segmentations = {}\n",
    "    for class_name, bboxes in yolo_bboxes.items():\n",
    "\n",
    "        for bbox in bboxes:\n",
    "\n",
    "            # Resize boundary\n",
    "            sam_resized_bbox = list(transform_bounding_box(bbox, 640, 640, target_size = 256))\n",
    "\n",
    "            # SAM inference\n",
    "            sam_inputs = sam_processor(sam_resized_image, input_boxes = [[sam_resized_bbox]], return_tensors = \"pt\").to('cuda')\n",
    "            sam_model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                sam_outputs = sam_model(**sam_inputs, multimask_output = False)\n",
    "\n",
    "            # Filter predicted map by 0.5\n",
    "            sam_seg_prob = torch.sigmoid(sam_outputs.pred_masks.squeeze(1))\n",
    "            sam_seg_prob = sam_seg_prob.cpu().numpy().squeeze()\n",
    "            sam_seg = (sam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "            if class_name not in yolo_sam_segmentations:\n",
    "                yolo_sam_segmentations[class_name] = sam_seg\n",
    "\n",
    "            else:\n",
    "                yolo_sam_segmentations[class_name] += sam_seg\n",
    "\n",
    "    #### Get GroundingDINO masks #####\n",
    "    groundingdino_sam_segmentations = {}\n",
    "    for class_name, bboxes in groundingdino_bboxes.items():\n",
    "\n",
    "        for bbox in bboxes:\n",
    "\n",
    "            # Resize boundary\n",
    "            sam_resized_bbox = list(transform_bounding_box(bbox, 640, 640, target_size = 256))\n",
    "\n",
    "            # SAM inference\n",
    "            sam_inputs = sam_processor(sam_resized_image, input_boxes = [[sam_resized_bbox]], return_tensors = \"pt\").to('cuda')\n",
    "            sam_model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                sam_outputs = sam_model(**sam_inputs, multimask_output = False)\n",
    "\n",
    "            # Filter predicted map by 0.5\n",
    "            sam_seg_prob = torch.sigmoid(sam_outputs.pred_masks.squeeze(1))\n",
    "            sam_seg_prob = sam_seg_prob.cpu().numpy().squeeze()\n",
    "            sam_seg = (sam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "            if class_name not in groundingdino_sam_segmentations:\n",
    "                groundingdino_sam_segmentations[class_name] = sam_seg\n",
    "\n",
    "            else:\n",
    "                groundingdino_sam_segmentations[class_name] += sam_seg\n",
    "\n",
    "    #### Get OWL-ViT masks #####\n",
    "    owlvit_sam_segmentations = {}\n",
    "    for class_name, bboxes in owlvittext_bboxes.items():\n",
    "\n",
    "        for bbox in bboxes:\n",
    "\n",
    "            # Resize boundary\n",
    "            sam_resized_bbox = list(transform_bounding_box(bbox, 640, 640, target_size = 256))\n",
    "\n",
    "            # SAM inference\n",
    "            sam_inputs = sam_processor(sam_resized_image, input_boxes = [[sam_resized_bbox]], return_tensors = \"pt\").to('cuda')\n",
    "            sam_model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                sam_outputs = sam_model(**sam_inputs, multimask_output = False)\n",
    "\n",
    "            # Filter predicted map by 0.5\n",
    "            sam_seg_prob = torch.sigmoid(sam_outputs.pred_masks.squeeze(1))\n",
    "            sam_seg_prob = sam_seg_prob.cpu().numpy().squeeze()\n",
    "            sam_seg = (sam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "\n",
    "            if class_name not in owlvit_sam_segmentations:\n",
    "                owlvit_sam_segmentations[class_name] = sam_seg\n",
    "\n",
    "            else:\n",
    "                owlvit_sam_segmentations[class_name] += sam_seg\n",
    "\n",
    "    # Combine different damage types\n",
    "    combined_damage_mask = {k:[] for k in ['dent', 'scratch', 'cracked damage', 'glass shatter', 'lamp broken', 'tire flat']}\n",
    "    for damage_type in ['dent', 'scratch', 'cracked damage', 'glass shatter', 'lamp broken', 'tire flat']:\n",
    "\n",
    "        total_mask = np.zeros((256, 256))\n",
    "\n",
    "        if damage_type in yolo_sam_segmentations:\n",
    "            total_mask += yolo_sam_segmentations[damage_type]\n",
    "\n",
    "        if damage_type in groundingdino_sam_segmentations:\n",
    "            total_mask += groundingdino_sam_segmentations[damage_type]\n",
    "\n",
    "        if damage_type in owlvit_sam_segmentations:\n",
    "            total_mask += owlvit_sam_segmentations[damage_type]\n",
    "\n",
    "        unique_mask_values = np.unique(total_mask.flatten())\n",
    "        if max(unique_mask_values) > 0:\n",
    "            normalized_mask = map_values(total_mask, [val for val in unique_mask_values if val >= int(max(unique_mask_values)/2)])\n",
    "        else:\n",
    "            normalized_mask = np.zeros((256, 256))\n",
    "\n",
    "        combined_damage_mask[damage_type] = normalized_mask\n",
    "\n",
    "    for label, mask in groundedtruth_segmentations.items():\n",
    "\n",
    "        if label in yolo_sam_segmentations:\n",
    "            iou = calculate_iou_mask(combined_damage_mask[label], mask)\n",
    "\n",
    "        else:\n",
    "            iou = None\n",
    "\n",
    "        testing_result = {'image_name':image_name, 'iou':iou}\n",
    "        testing_results.append(testing_result)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RjXh-7FcgnQ8",
    "0WPwQLx1g3MS",
    "r0rn3GqxjyEs",
    "eXFSdIZnlC43",
    "Yz5G86IJL8GQ",
    "nanRGsafmBKv",
    "jo4VOs5v35MX",
    "RE2kw7PJpMHL"
   ],
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
